{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indonesian-birthday",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artistic-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-telling",
   "metadata": {},
   "source": [
    "#### NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "nuclear-vacuum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\richa\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# setup splinter and bs4\n",
    "executable_path = {\"executable_path\": ChromeDriverManager().install()}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=True)\n",
    "\n",
    "url = \"https://redplanetscience.com/\"\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# scrape the Mars News Site and collect the latest News Title and Paragraph Text. \n",
    "# assign the text to variables that you can reference later.\n",
    "news_title = soup.find('div', id='news').find('div', class_='content_title').text\n",
    "news_p = soup.find('div', id='news').find('div', class_='article_teaser_body').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-coordinate",
   "metadata": {},
   "source": [
    "#### JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "furnished-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit the url for the Featured Space Image site.\n",
    "# use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "# make sure to find the image url to the full size .jpg image.\n",
    "# make sure to save a complete url string for this image.\n",
    "url = \"https://spaceimages-mars.com/\"\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "featured_image_url = url + soup.find('img', class_='headerimage fade-in')['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-powell",
   "metadata": {},
   "source": [
    "#### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accomplished-anthropology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visit the Mars Facts webpage and use Pandas to scrape the table containing facts about the planet\n",
    "# including Diameter, Mass, etc.\n",
    "# use Pandas to convert the data to a HTML table string.\n",
    "url = \"https://galaxyfacts-mars.com/\"\n",
    "tables = pd.read_html(url)\n",
    "df = tables[0]\n",
    "df.columns = df.iloc[0]\n",
    "df = df[1:]\n",
    "df.columns = ['', 'Mars', 'Earth']\n",
    "table_string = df.to_html(index=False, classes='table table-striped table-hover')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-undergraduate",
   "metadata": {},
   "source": [
    "#### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "alien-translator",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Cerberus Hemisphere Enhanced\n",
      "https://marshemispheres.com/cerberus.html\n",
      "1\n",
      "Schiaparelli Hemisphere Enhanced\n",
      "https://marshemispheres.com/schiaparelli.html\n",
      "2\n",
      "Syrtis Major Hemisphere Enhanced\n",
      "https://marshemispheres.com/syrtis.html\n",
      "3\n",
      "Valles Marineris Hemisphere Enhanced\n",
      "https://marshemispheres.com/valles.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/schiaparelli_enhanced-full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/syrtis_major_enhanced-full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/valles_marineris_enhanced-full.jpg'}]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visit the astrogeology site to obtain high resolution images for each of Mar's hemispheres.\n",
    "# you will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "# save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "# append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "url = \"https://marshemispheres.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for i in range(4):\n",
    "    items = soup.find_all('div', class_='description')\n",
    "    title = items[i].h3.text\n",
    "    link = title.split()[0].lower() + '.html'\n",
    "    new_url = url + link\n",
    "    response = requests.get(new_url)\n",
    "    soup_new = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_url = url + soup_new.find('div', class_='downloads').a['href']\n",
    "    hemisphere_image_urls.append({'title': title, 'img_url': img_url})\n",
    "\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in a dictionary\n",
    "mars_dict = {\n",
    "    \"news_title\": news_title,\n",
    "    \"news_p\": news_p,\n",
    "    \"featured_image_url\": featured_image_url,\n",
    "    \"table_string\": table_string,\n",
    "    \"hemisphere_1\": hemisphere_image_urls[0],\n",
    "    \"hemisphere_2\": hemisphere_image_urls[1],\n",
    "    \"hemisphere_3\": hemisphere_image_urls[2],\n",
    "    \"hemisphere_4\": hemisphere_image_urls[3],\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pythonData] *",
   "language": "python",
   "name": "conda-env-pythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
